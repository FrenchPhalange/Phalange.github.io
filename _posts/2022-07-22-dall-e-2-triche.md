---
layout: post
title:  "La magouille d'OpenAI pour am√©liorer la diversit√© de DALL¬∑E 2"
date:   2022-07-22 12:24:12 +0200
image:  '/images/blog/dall-e-2-triche/header.png'
tags:   [IA]
---

Cette semaine, la soci√©t√© **OpenAI** a d√©ploy√© une mise √† jour de son c√©l√®bre r√©seau de neurones **DALL¬∑E** pour am√©liorer la diversit√© des images g√©n√©r√©es par celui-ci, mais... ils ont trich√© !

Dans ce thread, on va parler des biais algorithmiques dans l'intelligence artificielle. üßµ 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/dall-e-2-triche/1550426475794501632-FYQy4suWYAAN8CF.jpg" draggable="false">
  </div>
</div>

Vous connaissez s√ªrement DALL¬∑E, cette IA r√©volutionnaire capable de g√©n√©rer des images correspondant √† n'importe quelle phrase, dont j'explique le fonctionnement dans [cet autre article](/blog/fonctionnement-dall-e-2).

Quelques exemples : 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/dall-e-2-triche/1550426483570753537-FYQzAq5XoAA98Xc.jpg" draggable="false">
  </div>
</div>

L'√©tude des biais occupe depuis quelques ann√©es une place tr√®s importante dans la recherche en intelligence artificielle et dans ses applications.

Mais c'est quoi exactement un biais algorithmique ?

Comme son nom l'indique, c'est la transposition d'un biais (sexiste, socio-√©conomique, raciste, ...) √† un algorithme de traitement automatis√© des donn√©es.

Ils apparaissent g√©n√©ralement au cours d'un processus d'apprentissage automatique, aussi appel√© machine learning. 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/dall-e-2-triche/1550426491929993216-FYQz3HAWYAIqO1Z.jpg" draggable="false">
  </div>
</div>

J'ai d'ailleurs un petit jeu pour vous √† la fin de cet article, pour vous montrer que tout le monde est affect√© par ses propres biais sans s'en rendre compte.

Restez jusqu'au bout si vous avez envie d'en apprendre plus sur votre cerveau üòâ

Une premi√®re cause des biais algorithmiques, c'est le **mauvais √©chantillonnage** du dataset.

Supposons que vous vouliez ouvrir un restaurant dans votre ville : si votre √©tude de march√© n'interroge que des enfants de 5 √† 10 ans, vous risquez d'avoir peu de l√©gumes verts au menu ! 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/dall-e-2-triche/1550426503489413126-FYQ0s12WYAMDl8q.jpg" draggable="false">
  </div>
</div>

Vous voulez un exemple dans le monde r√©el ?

En 2015, l'IA de Google Photos charg√©e de cat√©goriser chaque prise de vue a mis l'√©tiquette "gorille" sur de nombreuses photos... de personnes noires. 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/dall-e-2-triche/1550426509290217473-FYQ01IpXgAAtpfa.png" draggable="false">
  </div>
</div>

Cet algorithme avait probablement √©t√© d√©velopp√© par une √©quipe am√©ricaine, et entra√Æn√© sur des donn√©es provenant d'internet.

Avec une probable **sous-repr√©sentation de personnes noires** dans le dataset d'entra√Ænement, on explique facilement cette grave erreur de classification. 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/dall-e-2-triche/1550426514281312257-FYQ1pCIXoAAw426.png" draggable="false">
  </div>
</div>

On se souviendra aussi de **Tay**, l'intelligence artificielle cr√©√©e par Microsoft qui am√©liorait son mod√®le de conversation √† partir des discussions qu'elle entretenait sur Twitter, et qui avait d√ª √™tre d√©sactiv√©e au bout de quelques jours √† cause de d√©rives assez gla√ßantes : 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/dall-e-2-triche/1550426518416932866-FYQ1W9dXoAAmkjK.jpg" draggable="false">
  </div>
</div>

(Je sais qu'il est discutable d'attribuer l'√©chec de Tay √† un biais d'√©chantillonnage plut√¥t qu'√† une d√©gradation volontaire des internautes, mais √ßa reste un probl√®me de dataset üòâ)

Le probl√®me d'apprentissage appara√Æt dans ces deux exemples de mani√®re √©vidente, tout comme la solution technique qui permet d'y rem√©dier.

Maintenant, observons un cas plus difficile √† appr√©hender.

Vous avez ici une capture d'√©cran de Google Translate que j'ai prise √† l'instant.

En passant de l'anglais au fran√ßais, on force l'algorithme √† faire un choix pour attribuer un genre √† chaque m√©tier : certains deviennent masculins, d'autres f√©minins. 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/dall-e-2-triche/1550426525438197761-FYQ18wNWQAAHYRh.jpg" draggable="false">
  </div>
</div>

Biais d'√©chantillonnage ? Sexisme ? Je pense que le probl√®me est plus complexe que √ßa.

Factuellement, il y a plus d'infirmi√®res que d'infirmiers dans le monde, tout comme il y a plus d'ing√©nieurs que d'ing√©nieures. L'algo ne fait donc que minimiser son erreur.

Ici, la probl√©matique n'est pas uniquement technologique, on entre dans un **d√©bat soci√©tal** : faut-il privil√©gier le statu quo au risque d'alimenter des st√©r√©otypes de genre, ou au contraire choisir la neutralit√© absolue pour favoriser la repr√©sentation de la diversit√© ? 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/dall-e-2-triche/1550426529624215552-FYQ2q1NXgAE1maL.jpg" draggable="false">
  </div>
</div>

Vous le voyez venir le d√©bat sur l'√©criture inclusive dans mes mentions ? üòÖ

Oui, c'est n√©cessaire de se poser ce genre de questions, car l'informatique a un potentiel √©norme sur le **progr√®s social** et c'est √† nous de faire √©voluer notre domaine de mani√®re r√©fl√©chie. Et pas la peine de bosser dans l'IA pour avoir un impact üòâ

Pour OpenAI, comme pour beaucoup d'autres entreprises et laboratoires qui bossent sur de l'intelligence artificielle, les biais algorithmiques sont un sujet majeur.

On imagine bien qu'un bad buzz sur un mod√®le raciste ou sexiste, √ßa ferait mal au business !

Pendant la version alpha de DALL¬∑E, l'entreprise a tout fait pour prot√©ger ses int√©r√™ts : dataset et mod√®le non publics, et un acc√®s r√©serv√© √† un petit ensemble de personnes contraintes par des CGU strictes sur le contenu qu'elles pouvaient g√©n√©rer et partager. 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/dall-e-2-triche/1550426542936887297-FYQ3hmYWAAAnrA-.jpg" draggable="false">
  </div>
</div>

√áa n'a pas emp√™ch√© certains biais algorithmiques d'√™tre mis en √©vidence. 

La plupart des images contenant des humains repr√©sentaient des personnes **blanches**, avec des **st√©r√©otypes de genre** tr√®s pr√©sents sur les m√©tiers : 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/dall-e-2-triche/1550426549010186242-FYQ3tAKXwAIIiZn.jpg" draggable="false">
  </div>
</div>

Pour rem√©dier √† tout √ßa, OpenAI a fait une annonce importante lundi : une nouvelle technique a √©t√© mise en place "pour que DALL¬∑E repr√©sente de mani√®re plus fid√®le la diversit√© de la population mondiale".

Aucun d√©tail fourni, mais de belles images pour montrer l'avant/apr√®s. 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/dall-e-2-triche/1550426554483761160-FYQ39PoXkAEWMJK.jpg" draggable="false">
  </div>
</div>

Il s'av√®re que la technique en question est une **belle arnaque** !

Au lieu de changer le dataset ou de d√©velopper une nouvelle m√©thode d'entra√Ænement, ils ajoutent discr√®tement des mots comme "black" ou "female" √† la fin de la phrase fournie au r√©seau de neurones üòÖ

Pour r√©v√©ler la supercherie, des chercheuses et chercheurs ayant acc√®s √† DALL¬∑E ont fourni √† l'IA des phrases incompl√®tes : "une personne tenant une pancarte o√π il est √©crit", ou encore "une personne avec un T-shirt qui dit".

La plupart du temps, les images g√©n√©r√©es ont un texte incompr√©hensible car DALL¬∑E est confus par ces phrases qui n'ont pas vraiment de sens. 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/dall-e-2-triche/1550426564663431168-FYQ4P5vWIAEBW6m.jpg" draggable="false">
  </div>
</div>

Mais de temps en temps, quand OpenAI choisit al√©atoirement d'ajouter un mot √† la fin de la phrase, **c'est magique** ! 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/dall-e-2-triche/1550426570401226753-FYQ4cZTWIAgHUPk.jpg" draggable="false">
  </div>
</div>

Alors pourquoi je critique cette d√©cision ? Dans les faits, c'est vrai que l'intention est bonne et que la solution fonctionne : la diversit√© per√ßue a √©t√© **multipli√©e par 12** selon OpenAI.

Mais ce choix technique rel√®ve de la facilit√©, et le manque de transparence sur son fonctionnement (malgr√© les nombreux communiqu√©s de presse de l'entreprise) laisse √† penser qu'il s'agit plus de **prot√©ger sa r√©putation** que de faire avancer la recherche.

Pire, dans certaines situations le biais se retrouve amplifi√© √† cause de ces ajouts de mots. Selon un utilisateur, la phrase "nurse" n'a g√©n√©r√© aucun homme parmi plusieurs essais, s√ªrement parce que la compensation du biais favorise explicitement la repr√©sentation de femmes. 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/dall-e-2-triche/1550427880114589696-FYQ5J6BWQAAzHVc.png" draggable="false">
  </div>
</div>

Alors on a parl√© des biais algorithmiques, mais vous pensez battre les ordinateurs sur ce domaine ? Je vais vous inviter √† faire un petit exercice que j'aime beaucoup : l'Implicit Association Test.

Dans ce test, vous devez classer rapidement des mots dans deux cat√©gories : art √† gauche, science √† droite. Puis c'est des mots masculins/f√©minins que vous devez trier. 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/dall-e-2-triche/1550427887383224323-FYQ5UYxXkAEnyxS.jpg" draggable="false">
  </div>
</div>

Et ensuite, on m√©lange les cat√©gories ! D'un c√¥t√© masculin + art, de l'autre f√©minin + science. Puis on inverse et on recommence. 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/dall-e-2-triche/1550427892580061184-FYQ5cFtXwAEeO6M.jpg" draggable="false">
  </div>
</div>

L'analyse du d√©lai de r√©ponse r√©v√®le que l'immense majorit√© des gens associent **plus facilement** les femmes aux disciplines artistiques, et les hommes √† la science.

Ces st√©r√©otypes sont souvent acquis inconsciemment, et se transmettent de mani√®re sociale et culturelle.

Pour tenter l'exp√©rience chez vous, c'est par ici (en anglais) : [https://implicit.harvard.edu/imp...](https://implicit.harvard.edu/implicit/takeatest.html)

Cliquez "I wish to proceed" en bas de la page, puis "Gender-Science IAT" (ou choisissez un autre type de biais).

Alors, √ßa calme hein ? 

<div class="gallery-box">
  <div class="gallery">
  <img src="/images/blog/dall-e-2-triche/1550427899332804608-FYQ5rgPWQAcU2L4.png" draggable="false">
  </div>
</div>

Pour conclure tout √ßa, **on vit dans un monde imparfait**. Tout est source de biais, m√™me nos ordinateurs qui sont finalement √† notre image.

Certains sont m√™me par nature impossibles √† effacer, mais n'oubliez pas : le plus important c'est d'√™tre **conscient** de leur pr√©sence.

J'esp√®re que cet article vous a plu ! Les biais dans l'informatique sont un sujet qui me tient √† coeur, notamment la repr√©sentation des femmes dans les m√©tiers de la tech. Si vous appr√©ciez ce que je fais, pensez √† partager : √ßa me permet de savoir ce qui vous pla√Æt et cr√©er davantage de contenus qui vous correspondent.
